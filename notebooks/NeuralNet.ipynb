{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Nerual Networks\n",
    "\n",
    "In this notebook, we show how neural networks described in chapter 5 of PRML can be implemented. \n",
    "\n",
    "Because the notation of the book has ambiguity, we first describe the formulation in detail, and then move to the corresponding code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "#to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setting\n",
    "\n",
    "Although we consider multiclass classification problem in our implementation, in Sections 1 and 2, we formulate the problem from broader perspectives, where we include regression, binary classification, and multiclass classification problems.\n",
    "\n",
    "Let us define symbols as follows: \n",
    "* $N \\in \\mathbb{N}$ : data size, \n",
    "* $d \\in \\mathbb{N}$ : the dimension of input, \n",
    "* Denote input data by $x_0, x_1, \\dots , x_{N-1} \\in \\mathbb{R}^{d}$. \n",
    "* For a regression problems, denote target data by $t_0, t_1, \\dots, t_{N-1} \\in \\mathbb{R}$. \n",
    "* For a binary classification problem, denote target data by $t_0, t_1, \\dots, t_{N-1} \\in \\{ 0, 1 \\}$.\n",
    "* For a multiclass classification problem, let $\\mathcal{C} = \\left\\{ 0, 1, \\dots, C-1 \\right\\}$ be the set of class labels, $t_0, t_1, \\dots, t_{N-1} \\in \\left\\{0,1\\right\\}^{C}$ be the 1-of-$C$ coding of target labels, i.e., if $n$-th label is $c$, $t_n$ is a vector with its $c$ th component being 1 and other components being zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Theory\n",
    "\n",
    "## 2.1 Model : the representation of neural networks\n",
    "\n",
    "We consider $L$-layer feedforward neural networks (feedforward neural networks with $L-1$ hidden layers) such as one shown in the following figure: \n",
    "<img src=\"NNfig.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "For simplicity, we assume that there is no skip-layer connection, i.e.,  each element in a layer is connected only to elements in its adjacent layer. \n",
    "\n",
    "In this section, we write down the definition of functions representing a neural network. Specifically, we decompose the function representing the neural network, so that we can extract contributions of parameters from a specified layer, for calculation of gradient in the next section, where we want derivative with respect to parameters from each layer. \n",
    "\n",
    "### 2.1.1 The number of elements and the dimension\n",
    "\n",
    "Let $n_l \\in \\mathbb{N}$ be the number of elements in the $l$-th layer. \n",
    "It follows that $n_0 = d$, and $n_L$ is the dimension of the output, which will be specified later.\n",
    "\n",
    "\n",
    "### 2.1.2 Activation functions\n",
    "\n",
    "Again for somplicity, we assume that activation functions for $l$-th layer with $l=1,2,\\dots,L-1$ are all \n",
    "$h : \\mathbb{R} \\rightarrow \\mathbb{R}$. $h$ can be, for example, logstic sigmoid function, tanh, ReLU and so on.\n",
    "Also, we define $h^{(l)}$ by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    h^{(l)} : \\mathbb{R}^{n_l} \\rightarrow \\mathbb{R}^{n_l}, \\ \\ \n",
    "    h^{(l)}(a) = (h(a_1), h(a_2), \\dots, h(a_{n_l}))^T \\ \\in \\mathbb{R}^{n_l}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us denote the activation function for the output layer by\n",
    "$g : \\mathbb{R}^{n_L} \\rightarrow \\mathbb{R}^{n_L}$. \n",
    "\n",
    "Concretely, $g$ can be chosen as below:\n",
    "* For binary classification, $g$ can be logistic sigmoid function, and $n_L = 1$. \n",
    "* For multiclass classification, $g$ can be softmax function, and $n_L = C$, where $C$ is the number of class labels.\n",
    "* For regression, $g$ can be an identity map, and $n_L = 1$.\n",
    "\n",
    "### 2.1.3 The input and output of each layer\n",
    "\n",
    "Let us define\n",
    "* $z^{(l)} \\in \\mathbb{R}^{n_l}$ : the output from the $l$-th layer\n",
    "* $a^{(l)} \\in \\mathbb{R}^{n_{l+1}}$ : the input to the $(l+1)$-th layer\n",
    "\n",
    "Then, \n",
    "$$\n",
    "\\begin{align}\n",
    "    z^{(l)} = h^{(l)}(a^{(l-1)})\n",
    "\\end{align}\n",
    "$$\n",
    "follows. \n",
    "\n",
    "Let us also define parameters as follows\n",
    "* $w^{(l)}_{i,j}$ : the weight of $j$-th element of $l$-th layer on the $i$-th element of$(l+1)$-th layer. \n",
    "* $b^{(l)}_{i}$ : the bias of $i$-th element of $(l+1)$-th layer.\n",
    "\n",
    "If we define \n",
    "$w^{(l)} = (w^{(l)}_{i,j})_{i,j}$ ($n_{l+1} \\times n_{l}$ matrix) and  \n",
    "$b^{(l)} = (b^{(l)}_{1}, b^{(l)}_{2}, \\dots, b^{(l)}_{n_{l+1}})^T$, \n",
    "then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    a^{(l)} = w^{(l)} z^{(l)} + b^{(l)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For later convenience, we denote this affine mapping by \n",
    "$$\n",
    "\\begin{align}\n",
    "    A^{(l)}_{\\theta^{(l)}} : \\mathbb{R}^{n_{l}} \\rightarrow \\mathbb{R}^{n_{l+1}}  , \\ \\ A^{(l)}_{\\theta^{(l)} }(z) := w^{(l)}z + b^{(l)}\n",
    "\\end{align}, \n",
    "$$\n",
    "where $\\theta^{(l)} := (w^{(l)}, b^{(l)})$.\n",
    "\n",
    "### 2.1.4 The whole network\n",
    "\n",
    "Let a function $f_{\\theta} : \\mathbb{R}^{n_0} \\rightarrow \\mathbb{R}^{n_L}$ be the function that represents the whole neural network we defined above. \n",
    "\n",
    "From the definitions above, it can be written as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_{\\theta} = g \\circ A^{(L-1)}_{\\theta^{(L-1)}} \\circ h^{(L-1)} \\circ A^{(L-2)}_{\\theta^{(L-2)}} \\circ \\cdots \n",
    "        A^{(l+1)}_{\\theta^{(l+1)}} \\circ h^{(l+1)} \\circ A^{(l)}_{\\theta^{(l)}} \\circ h^{(l)} \\circ A^{(l-1)}_{\\theta^{(l-1)}} \n",
    "        \\circ \\cdots \\circ h^{(1)} \\circ A^{(0)}_{\\theta^{(0)}} . \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "If we focus on parameters $\\theta^{(l)}$, we can decompose the function as \n",
    "$$\n",
    "\\begin{align}\n",
    "    f_{\\theta} = F^{(l)} \\circ A^{(l)}_{\\theta^{(l)}}  \\circ Z^{(l)} \n",
    "    \\ \\ (l = 0, \\dots, L-1), \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    F^{(l)} &:= g \\circ A^{(L-1)}_{\\theta^{(L-1)}} \\circ h^{(L-1)} \\circ A^{(L-2)}_{\\theta^{(L-2)}} \\circ \\cdots \n",
    "        A^{(l+1)}_{\\theta^{(l+1)}} \\circ h^{(l+1)}  \\ \\ (l = 0,1, \\dots, L-2) \\\\\n",
    "    F^{(L-1)} &:= g \\\\\n",
    "    Z^{(l)} &:= h^{(l)} \\circ A^{(l-1)}_{\\theta^{(l-1)}} \\circ \\cdots \\circ h^{(1)} \\circ A^{(0)}_{\\theta^{(0)}}  \\ \\ (l = 1, \\dots, L-1)\\\\\n",
    "    Z^{(0)} &:= I\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that in the representation $f_{\\theta} = F^{(l)} \\circ A^{(l)}_{\\theta^{(l)}}  \\circ Z^{(l)}$, \n",
    "$A^{(l)}_{\\theta^{(l)}}$ is the only part that contains $\\theta^{(l)}$, \n",
    "which will simplify our calculation of gradients in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cost functions and its gradient\n",
    "\n",
    "### 2.2.1 Cost functions\n",
    "\n",
    "We assume that, in training the neural network, we minimize the following cost function\n",
    "$$\n",
    "\\begin{align}\n",
    "    E_{tot}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} E \\left( f_{\\theta}(x_n), t_n \\right) + \\frac{\\lambda}{2N} \\|\\theta \\|^2 , \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\lambda$ is the regularization parameter, \n",
    "and $E$ is chosen appropriately depending on the problem: \n",
    "* For regression problem, $E$ can be $E(y,t) = \\frac{1}{2} \\| y - t \\|^2$ and so on.\n",
    "* For multiclass classification problem with $g$ being softmax functions, E can be $E(y,t) = -\\sum_{k=0}^{d_{out}-1} t_k \\log y_k$ (negative log likelihood).\n",
    "\n",
    "### 2.2.2 Gradient \n",
    "\n",
    "Here, we calculate the gradient of the cost function with respect to parameters $w, b$. \n",
    "We consider the regularization term separately, and concentrate on the first term. \n",
    "We can treat each data point separately because this term has the form of summation over data.\n",
    "\n",
    "The gradient for each term can be calculated as follows\n",
    "($l = 0,1,\\dots, L-1$, $i=0, 1, \\dots, n_{l+1}-1$, $j=0, 1, \\dots, n_{l}-1$):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\delta^{(l)}_{i} &:= \\sum_{k=0}^{n_L-1} \\left. \\frac{\\partial E}{\\partial y_k} \\right|_{y=f_{\\theta}(x)} \\cdot \n",
    "        \\left. \\frac{\\partial F^{(l)}_{k}}{\\partial a^{(l)}_{i}} \\right|_{a^{(l)}= (A^{(l)} \\circ Z^{(l)})(x) } \\\\\n",
    "    \\frac{\\partial}{\\partial w^{(l)}_{i,j}} E \\left( f_{\\theta}(x), t \\right) &= \\delta^{(l)}_{i} Z^{(l)}_{j}(x) \\\\\n",
    "    \\frac{\\partial}{\\partial b^{(l)}_{i}} E \\left( f_{\\theta}(x), t \\right) &= \\delta^{(l)}_{i} \n",
    "\\end{align}\n",
    "$$\n",
    "which can be derived straightforwardly from the definition of $f_{\\theta}$ and the cost function given above.\n",
    "\n",
    "Thus, to obtain the gradient, it is sufficient to calculate the quantities $\\delta^{(l)}_{i}$. \n",
    "Here, $\\delta^{(l)}_{i}$ represents how much the cost function changes when $a^{(l)}_{i}$, the input to the $i$-th element of $(l+1)$-th layer, changes slightly. We may denote $\\delta^{(l)}_{i}$ ($i=0,1, \\dots, n_{l+1}-1$) collectively by $\\delta^{(l)}$.\n",
    "\n",
    "In the next section, we discuss back propagation, which is an algorithm to calculate these quantities recursively. \n",
    "\n",
    "\n",
    "### 2.2.3 Back propagation\n",
    "\n",
    "In back propagation procedure, we calculate $\\delta^{(l)}_{i}$, begining from $l=L-1$, and all the way down to $l=0$. \n",
    "\n",
    "First, for $l=L-1$, we have $F^{(L-1)} = g$, and hence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\delta^{(L-1)}_{i} = \\sum_{k=0}^{n_L-1} \\left. \\frac{\\partial E}{\\partial y_k} \\right|_{y=f_{\\theta}(x)} \\cdot \n",
    "        \\left. \\frac{\\partial g_{k}}{\\partial a^{(L-1)}_{i}} \\right|_{a^{(L-1)}= (A^{(L-1)} \\circ Z^{(L-1)})(x) }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This can be easily calculated for various problems : \n",
    "* For least square regression, we have $E(y,t) = \\frac{1}{2} \\| y - t \\|^2$, $g(a) = a$, and hence $\\delta^{(L-1)}_{i} = (f_{\\theta}(x) - t)_{i}$. \n",
    "* For multiclass classification with $E(y,t) = -\\sum_{k=0}^{d_{out}-1} t_k \\log y_k$, $g_k(a) = \\frac{e^{a_k}}{\\sum_{m} e^{a_m}}$, we obtain $\\delta^{(L-1)}_{i} = (f_{\\theta}(x) - t)_{i}$. \n",
    "\n",
    "Next, assume that we have obtained $\\delta^{(l+1)}$ for a specific $l \\leq L - 2$. \n",
    "We can derive a recursion equation where $\\delta^{(l)}$ can be calculated from $\\delta^{(l+1)}$ as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\delta^{(l)}_{i} = h'(a^{(l)}_{i}) \\sum_{j=0}^{n_{l+2}-1} \\delta^{(l+1)}_{j} w^{(l+1)}_{j,i}\n",
    "\\end{align}\n",
    "$$\n",
    "This equation can be drived easily by noting the relation $F^{(l)} = F^{(l+1)} \\circ A^{(l+1)}_{\\theta^{(l+1)}} \\circ h^{(l+1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 More on backpropagation\n",
    "\n",
    "Having described the theory of back propagation in the previous section, we move a step forward to implement the idea.\n",
    "\n",
    "### 2.3.1 Change of notations\n",
    "\n",
    "From now on, to make the implementation simple, we treat weight $w$ and bias $b$ equally by defining \n",
    "$$\n",
    "\\begin{align}\n",
    "    W^{(l)}_{i,j} = \n",
    "    \\begin{cases}\n",
    "        b^{(l)}_{i} & (j=0) \\\\\n",
    "        w^{(l)}_{i,j-1} & (j = 1,2, \\dots, n_{l})\n",
    "    \\end{cases}\n",
    "    \\ \\ (i = 0,1, \\dots,n_{l+1} -1 ) , \n",
    "\\end{align}\n",
    "$$\n",
    "where $W^{(l)}$ is $n_{l+1} \\times (n_{l}+1)$ matrix, and \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tilde{z}^{(l)}_{i} = \n",
    "    \\begin{cases}\n",
    "        1 & (i=0) \\\\\n",
    "        z^{(l)}_{i-1} & (i = 1, \\dots, n_l)\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "With these notations, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "    a^{(l)} &= W^{(l)} \\tilde{z}^{(l)} \\\\\n",
    "    \\tilde{z}^{(l)} &= \n",
    "        \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            z^{(l)}\n",
    "        \\end{pmatrix} \\\\\n",
    "    \\frac{\\partial}{\\partial W^{(l)}_{i,j}} E(f_{\\theta}(x),t) &= \\delta^{(l)}_{i} \\tilde{z}^{(l)}_{j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 2.3.2 Gradient for a single data point\n",
    "\n",
    "For a data point $(x, t)$ we perform the following calculation : \n",
    "\n",
    "* Calculate $a, z$ for each layer (forward propagation) by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    a^{(l)} &= W^{(l)} \\tilde{z}^{(l)}  \\ \\ (\\tilde{z}^{(0)} := (1,x^T)^T) \\ \\ (l=0, 1, \\dots, L-1) ,\\\\\n",
    "     \\tilde{z}^{(l)} &=\n",
    "        \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            z^{(l)}\n",
    "        \\end{pmatrix}\\ \\ (l=1,\\dots, L-1) \\\\\n",
    "    y &= g(a^{(L-1)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Calculate $\\delta^{(L-1)}$. For examples described in section 3.3, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta^{(L-1)}_i = (y - t)_i . \n",
    "\\end{equation}\n",
    "\n",
    "* Recursively calculate $\\delta^{(l)}_{i}$ ($l= L-2, L-1, \\dots, 1, 0$) by \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta^{(l)}_{i} = h'(a^{(l)}_{i}) \\sum_{j=1}^{n_{l+2}} \\delta^{(l+1)}_{j} w^{(l+1)}_{j,i}\n",
    "\\end{equation}\n",
    "where $i = 0,1, \\dots, n_{l+1} - 1$\n",
    "\n",
    "* Calculate the gradient using $\\delta$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\frac{\\partial}{\\partial W^{(l)}_{i,j}} E(f_{\\theta}(x),t) &=& \\delta^{(l)}_{i} \\tilde{z}^{(l)}_{j}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Optimization by gradient descent\n",
    "\n",
    "To numerically minimize the cost function, here we use gradient descent method.\n",
    "\n",
    "In gradient descent method, we iteratively update our variable in the direction of gradient, i.e., if we denote the function to be minimized by $f$ and its variable by $x$, then in the $n$-th step of iteration, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "    x_{n+1} = x_n - \\alpha \\nabla f(x_n)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\alpha > 0$ is learning rate, which should be specified by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 From math to code\n",
    "\n",
    "Now, we translate our equations described above into codes. \n",
    "To perform computation efficiently, we utilize numpy array operation so that we do not have to code iteration over samples explicitly.\n",
    "\n",
    "\n",
    "## 3.1 Overview \n",
    "\n",
    "Before delving into the detail, let me give an overview: \n",
    "\n",
    "* First, we list up some variables, and prepare some utility functions in Section 3.2.\n",
    "* Next, we define functions performing forward propagation (`fprop`) and backward propagation (`bprop`) in Sections 3.3 and 3.4 respectively. Section 3.5 deals with the cost function and its gradient.\n",
    "* Then, we will define a function performing minimization using gradient descent method in Section 3.6.\n",
    "* Finally, in Section 3.7, we write `FFNN` class, which represents our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Variables\n",
    "\n",
    "Let us define some variable as follows.\n",
    "\n",
    "* `num_neurons` : ($L+1$,) array, where `num_neurons[l] = ` $n_{l}$ with $n_{0} = d$.\n",
    "* `act_func` : activation function $h$.\n",
    "* `act_func_deriv` : the derivative of the activation function, $h'$.\n",
    "* `out_act_func` : the activation function for the output layer, $g$.\n",
    "* `E` : a function representing the first term of the cost function, or more concretely $\\frac{1}{N} \\sum_{n=0}^{N-1}E(y_n, t_n)$\n",
    "* `lam` : the regularization constant $\\lambda$\n",
    "* `W_matrices` : ($L$,) array, with `dtype='object'`, `W_matrices[l][i, j]` = $W^{(l)}_{i,j}$, \n",
    "\n",
    "The variables shown above mainly concerns the structure and the contents of neural networks.\n",
    "\n",
    "Let us define some auxiliary matrices which depends on input data as follows:\n",
    "* `X` : ($N, d_{in}$) array. The input data, where `X[n, i]` $= X_{n, i} :=(x_{n})_{i}$\n",
    "* `C` : int, the number of categories we deal with.\n",
    "* `y` : ($N$,) array. The label data. Each element of `y` should be one of `0,1, ... , C-1`.\n",
    "* `T` : ($N, d_{out}$) array. The 1-of-C coding of the label data, where `T[n, i]` $= T_{n, i} := (t_{n})_{i}$\n",
    "* `Y` : ($N, d_{out}$) array. `Y[n, i]` = $Y_{n, i} := $the $i$-th element of the output from the neural network given the input $x_{n}$.\n",
    "* `A_matrices` : ($L$,) array, with `dtype='object'`, `A_matrices[l][n, i]` = $A^{(l)}_{n, i}$ := ($a^{(l)}_{i}$ for the $n$-th data point $x_n$). Note that $A^{(l)}$ is a $(N, n_{l+1})$ array.\n",
    "* `Z_matrices` : ($L$,) array, with `dtype='object'`, `z_matrices[l][n, i]` = $Z^{(l)}_{n, i}$ := ($\\tilde{z}^{(l)}_{i}$ for the $n$-th data point $x_n$). Note that $Z^{(l)}$ is a $(N, n_l+1)$ array.\n",
    "* `D_matrices` : ($L$,) array, with `dtype='object'`, `D_matrices[l][n, i]` = $\\Delta^{(l)}_{n,i} := (\\delta^{(l)}_{i}$ for the $n$-th data point $(x_n, t_n)$). Note that $\\Delta^{(l)}$ is a $(N, n_{l+1})$ array.\n",
    "\n",
    "\n",
    "We included the subscript $n$ in each matrix, because we want to iterate over data points by taking advantage of numpy array operation (For detail, see the next section.).\n",
    "\n",
    "Frist, we define a function which calculate 1-of-C encoding of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcTmat(y, C):\n",
    "    '''\n",
    "    This function generates the matrix T from the training label y and the number of classes C\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : 1-D numpy array\n",
    "        The elements of y should be integers in [0, C-1]\n",
    "    C : int\n",
    "        The number of classes\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    T : (len(y), C) numpy array\n",
    "        T[n, c] = 1 if y[n] == c else 0\n",
    "    '''\n",
    "    N = len(y)\n",
    "    T = np.zeros((N, C))\n",
    "    for c in range(C):\n",
    "        T[:, c] = (y == c)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We defined `W_matrices` as a ($L$,) array, whose element `W_matrices[l]` is a $(n_{l+1}, n_l + 1)$ array.\n",
    "However, for minimization process, it is easier to handle vector variables. \n",
    "Thus, for later convenience, here we define a function which transforms the 1-D array of arrays into a single vector and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_vec2mat(vec, num_neurons):\n",
    "    '''\n",
    "    vec : 1-D numpy array, dtype float\n",
    "        Vector, which we will convert into an array of matrices\n",
    "    num_neurons : 1-D array, dtype int\n",
    "        An array representing the structure of a neural network\n",
    "    '''\n",
    "    L = len(num_neurons) - 1\n",
    "    matrices = np.zeros(L, dtype='object')\n",
    "    ind = 0\n",
    "    for l in range(L):\n",
    "        mat_size = num_neurons[l+1] * (num_neurons[l] + 1)\n",
    "        matrices[l] = np.reshape(vec[ind: ind + mat_size], ( num_neurons[l+1], num_neurons[l] + 1) )\n",
    "        ind += mat_size        \n",
    "    return matrices\n",
    "\n",
    "def reshape_mat2vec(matrices, num_neurons):\n",
    "    '''\n",
    "    matrices : 1-D numpy array, dtype object\n",
    "        An array of matrices (2-D numpy array), which we will convert into a flattened 1-D array of float.\n",
    "    num_neurons : 1-D array, dtype int\n",
    "        An array representing the structure of a neural network\n",
    "    '''\n",
    "    L = len(num_neurons) - 1\n",
    "    vec = np.zeros( np.sum(num_neurons[1:]*(num_neurons[:L] + 1)) )\n",
    "    ind = 0\n",
    "    for l in range(L):\n",
    "        mat_size = num_neurons[l+1]*(num_neurons[l] + 1)\n",
    "        vec[ind:ind+mat_size] = np.reshape(matrices[l], mat_size)\n",
    "        ind += mat_size\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 forward propagation\n",
    "\n",
    "In forward propagation, we calculate $A^{(l)}$ and $Z^{(l)}$ ($l=0,1,\\dots, L-1$) by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    Z^{(l)} &= \\begin{cases}\n",
    "        \\left(\\boldsymbol{1}_{N}, X \\right) & (l=0) \\\\ \n",
    "        \\left(\\boldsymbol{1}_{N} , h\\left(A^{(l-1)} \\right)\\right) & (l = 1, \\dots, L-1) \n",
    "    \\end{cases}\\\\\n",
    "    A^{(l)} &= Z^{(l)} {W^{(l)}}^{T}\n",
    "\\end{align}\n",
    "$$\n",
    "and the final output $Y$ by \n",
    "$$\n",
    "\\begin{align}\n",
    "    Y = g(A^{(L-1)}), \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\boldsymbol{1}_N := (1, 1, \\dots, 1)^T \\in \\mathbb{R}^{N}$, \n",
    "and we slightly abuse notations so that we regard $h$ as acting elementwisely on $A^{(l)}$, \n",
    "and $Y = g(A^{(L-1)})$ means $Y_n = g(A^{L-1}_{n})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprop(W_matrices, act_func, out_act_func, X):\n",
    "    '''\n",
    "    This function performs forward propagation, and calculate input/output for each layer of a neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    W_matrices : 1-D array, dtype='object'\n",
    "        (L,) array, where W_matrices[l] corresponds to the parameters of l-th layer.\n",
    "    act_func : callable\n",
    "        activation function for hidden layers\n",
    "    out_act_func : callable\n",
    "        activation function for the final layer\n",
    "    X : 2-D array\n",
    "        (N,d) numpy array, with X[n, i] = i-th element of x_n\n",
    "        \n",
    "    Returns:\n",
    "    ----------\n",
    "    Z_matrices : 1-D array, dtype='object'\n",
    "        Z_matrices[l] is 1-D array, which represents the output from the l-th layer.\n",
    "    A_matrices : 1-D array, dtype='object'\n",
    "        A_matrices[l] is 1-D array, which represents the input to the (l+1)-th layer.\n",
    "    Y : 2-D array\n",
    "        The final output of the neural network.\n",
    "    '''\n",
    "    L = len(W_matrices)\n",
    "    N = len(X)\n",
    "    Z_matrices = np.zeros(L, dtype='object')\n",
    "    A_matrices = np.zeros(L, dtype='object')\n",
    "\n",
    "    Z_matrices[0] = np.concatenate((np.ones((N, 1)), X), axis=1)\n",
    "    A_matrices[0] = Z_matrices[0] @ (W_matrices[0].T)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z_matrices[l] = np.concatenate((np.ones((N, 1)), act_func(A_matrices[l-1])), axis=1)\n",
    "        A_matrices[l] = Z_matrices[l] @ (W_matrices[l].T)\n",
    "    Y = out_act_func(A_matrices[L-1])\n",
    "    \n",
    "    return Z_matrices, A_matrices, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Backward propagation\n",
    "\n",
    "In backward propagation, we calculate $\\Delta^{(L-1)}$ as follows: \n",
    "\n",
    "\n",
    "* First, calculate $\\Delta^{(L-1)}$. For examples described in section 3.3 we have $\\Delta^{(L-1)} = Y - T$. However, note that in general  $\\Delta^{(L-1)}$ may depend on $A$ and $Z$. In our implementation, `d_final_layer` takes care of the relation between $\\Delta^{(L-1)}$ and $A, Z, Y$ and $T$.\n",
    "* Recursively calculate $\\Delta^{(l)}$ ($l= L-2, L-1, \\dots, 1, 0$) by \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\Delta^{(l)} = h'(A^{(l)}) \\ast \\left( \\Delta^{(l+1)} w^{(l+1)}\\right), \n",
    "\\end{align}\n",
    "$$\n",
    "where we again slightly abused the definition of $h$, and $\\ast$ stands for elementwise multiplication. Note that we used $w^{(l)}$ not $W^{(l)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bprop(W_matrices, act_func, act_func_deriv, out_act_func, Z_matrices, A_matrices, Y, T, d_final_layer):\n",
    "    '''\n",
    "    This function performs backward propagation and returns delta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W_matrices : 1-D array, dtype='object'\n",
    "        (L,) array, where W_matrices[l] corresponds to the parameters of l-th layer.\n",
    "    act_func : callable\n",
    "        activation function for hidden layers\n",
    "    act_func_deriv : callable\n",
    "        The derivative of act_func\n",
    "    out_act_func : callable\n",
    "        activation function for the final layer\n",
    "    Z_matrices : 1-D array, dtype='object'\n",
    "        Z_matrices[l] is 1-D array, which represents the output from the l-th layer.\n",
    "    A_matrices : 1-D array, dtype='object'\n",
    "        A_matrices[l] is 1-D array, which represents the input to the (l+1)-th layer.\n",
    "    Y : 2-D array\n",
    "        The final output of the neural network.\n",
    "    T : 2-D array\n",
    "        Array for the label data.\n",
    "    d_final_layer : callable\n",
    "        A function of Z_matrices, A_matrices, Y and T, which returns $delta^{(L-1)}$\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    D_matrices : 1-D array\n",
    "        D_matrices[l] is a 2-D array, where D_matrices[l][n, i] = $\\Delta^{(l)}_{n, i}$\n",
    "\n",
    "    '''\n",
    "    L = len(W_matrices)\n",
    "    D_matrices = np.zeros(L, dtype='object')\n",
    "    \n",
    "    D_matrices[L-1] = d_final_layer(Z_matrices, A_matrices, Y, T)\n",
    "    for l in range(L-2, -1, -1):\n",
    "        D_matrices[l] = act_func_deriv(A_matrices[l]) * ( D_matrices[l+1] @ W_matrices[l+1][:, 1:] )\n",
    "\n",
    "    return D_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Cost function and its gradient\n",
    "\n",
    "Here we implement the function giving the value of the cost function and its gradient.\n",
    "\n",
    "Recall that the cost function is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "    E_{tot}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} E \\left( f_{\\theta}(x_n), t_n \\right) + \\frac{\\lambda}{2N} \\|\\theta \\|^2. \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradient of the first term can be obtained by \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial W^{(l)}_{i,j}} \\frac{1}{N}\\sum_{n=0}^{N-1} E(f_{\\theta}(x_n),t_n) \n",
    "    &= \\frac{1}{N}\\sum_{n=0}^{N-1} \\Delta^{(l)}_{n,i} Z^{(l)}_{n,j} \\\\\n",
    "    &= \\frac{1}{N} \\left( \\Delta^{(l)T} Z^{(l)} \\right)_{i,j}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_and_grad(W_matrices, X, T, act_func, act_func_deriv, out_act_func, E, d_final_layer, lam):\n",
    "    '''\n",
    "    This function performs backward propagation and returns delta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W_matrices : 1-D array, dtype='object'\n",
    "        (L,) array, where W_matrices[l] corresponds to the parameters of l-th layer.\n",
    "    X : 2-D array\n",
    "        (N,d) numpy array, with X[n, i] = i-th element of x_n    \n",
    "    T : 2-D array\n",
    "        Array for the label data.\n",
    "    act_func : callable\n",
    "        activation function for hidden layers\n",
    "    act_func_deriv : callable\n",
    "        The derivative of act_func\n",
    "    out_act_func : callable\n",
    "        activation function for the final layer\n",
    "    E : callable\n",
    "        A function representing the first term of the cost function\n",
    "    d_final_layer : callable\n",
    "        A function of Z_matrices, A_matrices, Y and T, which returns $delta^{(L-1)}$\n",
    "    lam : float\n",
    "        Regularization constant\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    cost_val : float\n",
    "        The value of the cost function\n",
    "    grad_matrices : 1-D array, dtype object\n",
    "        Array of 2-D matrices corresponding the gradient\n",
    "    '''\n",
    "    \n",
    "    N = len(X)\n",
    "    L = len(W_matrices)\n",
    "    \n",
    "    Z_matrices, A_matrices, Y = fprop(\n",
    "        W_matrices=W_matrices, \n",
    "        act_func=act_func, \n",
    "        out_act_func=out_act_func, \n",
    "        X=X)\n",
    "    \n",
    "    cost_val = E(Y, T) + lam/(2*N)*( np.sum( np.linalg.norm(W)**2 for W in W_matrices ) )\n",
    "    \n",
    "    D_matrices = bprop(\n",
    "        W_matrices=W_matrices, \n",
    "        act_func=act_func, \n",
    "        act_func_deriv=act_func_deriv, \n",
    "        out_act_func=out_act_func, \n",
    "        Z_matrices=Z_matrices, \n",
    "        A_matrices=A_matrices, \n",
    "        Y=Y, \n",
    "        T=T, \n",
    "        d_final_layer=d_final_layer)\n",
    "    \n",
    "    grad_matrices = np.zeros(L, dtype='object')\n",
    "    for l in range(L):\n",
    "        grad_matrices[l] = D_matrices[l].T @ Z_matrices[l] / N + lam/N * W_matrices[l]\n",
    "    \n",
    "    return cost_val, grad_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Minimization by gradient descent\n",
    "\n",
    "In gradient descent, we obtain an approximate optimal solution by the update\n",
    "$$\n",
    "\\begin{align}\n",
    "    x_{n+1} = x_n - \\alpha \\nabla f(x_n).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, as to the stopping criterion, we examine the change of function value, and if it does not decrease more than the specified value `ftol`, we terminate the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_GD(func_and_grad, x0, alpha=0.01, maxiter=1e4, ftol=1e-5):\n",
    "    '''\n",
    "    This function minimizes the given function using gradient descent method\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func_and_grad : callable\n",
    "        A function which returns the tuple (value of function to be minimized (real-valued), gradient of the function)\n",
    "    x0 : 1-D array\n",
    "        Initial value of the variable\n",
    "    alpha : float\n",
    "        Learning rate\n",
    "    maxiter : int\n",
    "        Maximum number of iteration\n",
    "    ftol : float\n",
    "        The threshold for stopping criterion. If the change of the value of function is smaller than this value, the iteration stops.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    result : dictionary\n",
    "        result['x'] ... variable, result['nit'] ... the number of iteration, result['func']...the value of the function, result['success']... whether the minimization is successful or not\n",
    "    '''\n",
    "    nit = 0\n",
    "    x = x0\n",
    "    val, grad = func_and_grad(x)\n",
    "    while nit < maxiter:\n",
    "        xold = x\n",
    "        valold = val\n",
    "        x = x - alpha*grad\n",
    "        val, grad = func_and_grad(x)\n",
    "        nit += 1\n",
    "        if abs(val - valold) < ftol:\n",
    "            break\n",
    "    success = (nit < maxiter)\n",
    "    return {'x': x, 'nit':nit, 'func':val, 'success':success}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Neural Network\n",
    "\n",
    "Because we consider multiclass classification problem here, we use conventional choice of activation functions, where activation function for hidden layers are logistic sigmoid function, the activation function for the output layer is softmax, and the error function is cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E(Y,T):\n",
    "    N = len(Y)\n",
    "    return -np.sum( T*np.log(Y) )/N\n",
    "\n",
    "def softmax(x):\n",
    "    x0 = np.max(x,axis=1)\n",
    "    x0 = np.reshape(x0, (len(x0),1))\n",
    "    tmp = np.sum(np.exp(x-x0),axis=1)\n",
    "    return np.exp(x-x0)/np.reshape(tmp,(len(tmp),1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "\n",
    "def d_final_layer(Z, A, Y, T):\n",
    "    return Y - T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the preparations, here we define our neural network, `FFNN` class (abbreviation of feed forward neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, C, lam, num_neurons, act_func=sigmoid, act_func_deriv=sigmoid_deriv, out_act_func=softmax, E=E, d_final_layer=d_final_layer):\n",
    "        self.C = C\n",
    "        self.num_neurons = num_neurons\n",
    "        self.L = len(num_neurons) - 1\n",
    "        self.act_func = act_func\n",
    "        self.act_func_deriv = act_func_deriv\n",
    "        self.out_act_func = out_act_func\n",
    "        self.E = E\n",
    "        self.d_final_layer = d_final_layer\n",
    "        self.lam = lam\n",
    "        self.W_matrices = np.zeros(self.L,dtype='object') \n",
    "        \n",
    "    def fit(self, X, y, ep=0.01, alpha=0.01, maxiter=1e4, ftol=1e-5, show_message=False): \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-D array\n",
    "            (N,d) numpy array, with X[n, i] = i-th element of x_n    \n",
    "        y : 1-D numpy array\n",
    "            The elements of y should be integers in [0, C-1]\n",
    "        ep : float\n",
    "            The maximum value of the initial weight parameter, which will be used for random initialization\n",
    "        alpha : float\n",
    "            Learning rate\n",
    "        maxiter : int\n",
    "            Maximum number of iteration\n",
    "        ftol : float\n",
    "            The threshold for stopping criterion. If the change of the value of function is smaller than this value, the iteration stops.\n",
    "        show_message : bool\n",
    "            If True, the result of the optimization is shown.\n",
    "        '''\n",
    "        T = calcTmat(y, self.C)\n",
    "        def cost_and_grad_vec(x):\n",
    "            val, grad_mat = cost_and_grad(\n",
    "                W_matrices=reshape_vec2mat(x, num_neurons=self.num_neurons),\n",
    "                X=X, \n",
    "                T=T,\n",
    "                act_func=self.act_func, \n",
    "                act_func_deriv=self.act_func_deriv,\n",
    "                out_act_func=self.out_act_func,\n",
    "                E=self.E,\n",
    "                d_final_layer=self.d_final_layer,\n",
    "                lam=self.lam\n",
    "            )\n",
    "            grad_vec = reshape_mat2vec(matrices=grad_mat, num_neurons=self.num_neurons)\n",
    "            return val, grad_vec\n",
    "        tht0 = 2*ep*(np.random.random(np.sum(self.num_neurons[1:]*(self.num_neurons[:self.L]+1))) - 0.5)\n",
    "        time_start = time.time()\n",
    "        result = minimize_GD(\n",
    "            func_and_grad=cost_and_grad_vec,\n",
    "            x0=tht0,\n",
    "            alpha=alpha,\n",
    "            maxiter=maxiter,\n",
    "            ftol=ftol\n",
    "        )\n",
    "        time_end = time.time()\n",
    "        if show_message:\n",
    "            print(f\"success : {result['success']}\")\n",
    "            print(f\"nit : {result['nit']}\")\n",
    "            print(f\"func value : {result['func']}\")\n",
    "            print(f\"calcualtion time : {time_end - time_start}seconds\")\n",
    "        self.W_matrices=reshape_vec2mat(vec=result['x'], num_neurons=self.num_neurons)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-D array\n",
    "            (N,d) numpy array, with X[n, i] = i-th element of x_n\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        Y : 2-D array\n",
    "            (len(X), self.C) array, where Y[n, c] represents the probability that the n-th instance belongs to c-th class\n",
    "        '''\n",
    "        Z_matrices, A_matrices, Y = fprop(\n",
    "            W_matrices=self.W_matrices, \n",
    "            act_func=self.act_func, \n",
    "            out_act_func=self.out_act_func, \n",
    "            X=X)\n",
    "        return Y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-D array\n",
    "            (N,d) numpy array, with X[n, i] = i-th element of x_n\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        classes : 1-D numpy array\n",
    "            (len(X), ) array, where classes[n] represents the predicted class to which the n-th instance belongs\n",
    "        '''\n",
    "        Y = self.predict_proba(X)\n",
    "        return np.argmax(Y, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Gradient check\n",
    "\n",
    "Because the implementation of the backward propagation is complicated, it is desirable to check whether the implementation is correct before we use the code. \n",
    "\n",
    "In this section, we compare the gradient obtained by the backward propagation and the gradient calculated from direct numerical differentiation. \n",
    "\n",
    "The numerical differentiation is obtained as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial E_{tot} }{\\partial W^{(l)}_{i,j}} \\simeq \n",
    "        \\frac{ E_{tot}(W^{(l)}_{i,j}+\\varepsilon) - E_{tot}(W^{(l)}_{i,j}-\\varepsilon) }{2\\varepsilon}, \n",
    "\\end{align}\n",
    "$$\n",
    "where we slightly abused the notation so that $E_{tot}(W^{(l)}_{i,j}\\pm\\varepsilon)$ means the value of the function, where all the elements, except for the $(l,i,j)$ element, of $W$ is fixed to $W^{(l')}_{i',j'}$, and the $(l,i,j)$ element is perturbed by $\\varepsilon$.\n",
    "The quantity $\\varepsilon$ is assumed to be small, and the error is $\\mathcal{O}(\\varepsilon)$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_check(nn, W_vec, X, T, ep):\n",
    "    num_params = len(W_vec)\n",
    "    W_matrices = reshape_vec2mat(W_vec, nn.num_neurons)\n",
    "    cost, grad_mat_bp = cost_and_grad(W_matrices, X, T, act_func=nn.act_func, act_func_deriv=nn.act_func_deriv, out_act_func=nn.out_act_func, E=nn.E, d_final_layer=nn.d_final_layer, lam=nn.lam)\n",
    "    grad_vec_bp = reshape_mat2vec(grad_mat_bp, nn.num_neurons)\n",
    "    \n",
    "    grad_vec_num = np.zeros(num_params)\n",
    "    for cnt in range(num_params):\n",
    "        W_p = np.copy(W_vec)\n",
    "        W_m = np.copy(W_vec)\n",
    "        W_p[cnt] += ep\n",
    "        W_m[cnt] -= ep\n",
    "        cost_p, grad_p = cost_and_grad(reshape_vec2mat(W_p, nn.num_neurons), X, T, act_func=nn.act_func, act_func_deriv=nn.act_func_deriv, out_act_func=nn.out_act_func, E=nn.E, d_final_layer=nn.d_final_layer, lam=nn.lam)\n",
    "        cost_m, grad_m = cost_and_grad(reshape_vec2mat(W_m, nn.num_neurons), X, T, act_func=nn.act_func, act_func_deriv=nn.act_func_deriv, out_act_func=nn.out_act_func, E=nn.E, d_final_layer=nn.d_final_layer, lam=nn.lam)\n",
    "        grad_vec_num[cnt] = (cost_p - cost_m )/(2*ep)\n",
    "    return np.linalg.norm(grad_vec_bp - grad_vec_num), grad_vec_bp, grad_vec_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code, we perform gradient check for a small neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.080169811959002e-10\n"
     ]
    }
   ],
   "source": [
    "num_neurons = np.array([4,3,2])\n",
    "\n",
    "L = len(num_neurons) - 1\n",
    "dim_W = np.sum(num_neurons[1:] * (num_neurons[:L]+1))\n",
    "nn = FFNN(C=3, lam=0.5, num_neurons=num_neurons)\n",
    "W_vec = np.arange(0.0, dim_W, 1)\n",
    "X = np.reshape(np.arange(0.0, 8.0, 1.0), (2,4))\n",
    "T = np.array([[1.0,0],[0,1]])\n",
    "error, grad_vec_bp, grad_vec_num = grad_check(nn, W_vec, X, T, ep=0.0001)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the difference between the two gradients, where one is obtained by backward propagation and the other by numerical differentiation, is sufficiently small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Toy data \n",
    "\n",
    "Having checked the validity of our implementation, here we apply our code to simple toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAKCiIjkKCCIiAgA/x/0T2B64zp1/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = datasets.make_blobs(n_samples=200, n_features=2, centers=3)\n",
    "for c in range(3):\n",
    "    plt.plot(X[:,0][y==c], X[:,1][y==c], 'o', label=f'{c}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(np.min(X[:,0]), np.max(X[:,0]),101)\n",
    "yy = np.linspace(np.min(X[:,1]), np.max(X[:,1]),100)\n",
    "Xtest = np.array([[x,y] for x in xx  for y in yy])\n",
    "xxx,yyy = np.meshgrid(xx, yy)\n",
    "\n",
    "def plot_result_blob(clf, C):\n",
    "    for c in range(C):\n",
    "        plt.plot(X[:,0][y==c], X[:,1][y==c],\"o\",label=f\"{c}\")\n",
    "    plt.legend()\n",
    "    pred_val = clf.predict(Xtest)\n",
    "    pred_val_2D = np.reshape(pred_val, (len(xx), len(yy))).T\n",
    "    plt.pcolormesh(xxx, yyy, pred_val_2D)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct our neural network with $L=2$ and $n_0 = 2, n_1 = 2, n_2 = 3$. Note that $n_0$ and $n_L$ is fixed once we fix the input dimension and the class numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success : True\n",
      "nit : 1295\n",
      "func value : 0.130879514461342\n",
      "calcualtion time : 0.5100293159484863seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHRjMShDlpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = FFNN(C=3, lam=0.5, num_neurons=np.array([2, 2, 3]))\n",
    "clf.fit(X, y, show_message=True, ep=0.1, alpha=0.1, ftol=1e-5)\n",
    "plot_result_blob(clf, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Hand-written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data : 1347\n",
      "Test data : 450\n",
      "Input dimension : 64\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "dat_train, dat_test, label_train, label_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "print(f\"Training data : {len(dat_train)}\")\n",
    "print(f\"Test data : {len(dat_test)}\")\n",
    "print(f\"Input dimension : {dat_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result_digit(clf):\n",
    "    label_test_pred = clf.predict(dat_test)\n",
    "    print(f\"train accuracy score: {accuracy_score(label_train, clf.predict(dat_train))}\")\n",
    "    print(f\"test accuracy score: {accuracy_score(label_test, label_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we try out $L=2$ neural network. We have to choose $n_0=64, n_2 = 10$. The choice of $n_1$ is arbitrary, but here we try $n_1 = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success : True\n",
      "nit : 2071\n",
      "func value : 0.05296457545742904\n",
      "calcualtion time : 7.4523024559021seconds\n"
     ]
    }
   ],
   "source": [
    "num_neurons = np.array([64,35,10])\n",
    "nn_mnist = FFNN(C=10, lam=0.5, num_neurons=num_neurons)\n",
    "nn_mnist.fit(dat_train, label_train, show_message=True, ep=0.01, alpha=0.1, ftol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy score: 0.9992576095025983\n",
      "test accuracy score: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "show_result_digit(nn_mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
