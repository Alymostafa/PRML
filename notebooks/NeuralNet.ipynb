{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Nerual Networks\n",
    "\n",
    "In this notebook, we show how neral networks described in chapter 5 of PRML can be implemented. \n",
    "\n",
    "Because the notation of the book is a little sloppy, we first describe the formulation in detail, \n",
    "and then move to the corresponding code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sokohaku\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import datasets, cross_validation, metrics\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc(\"savefig\",dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setting\n",
    "\n",
    "Let us define symbols as below: \n",
    "* $N \\in \\mathbb{N}$ : data size, \n",
    "* $d \\in \\mathbb{N}$ : the dimension of input, \n",
    "* $d_{out} \\in \\mathbb{N}$ : the dimension of output, \n",
    "* Denote input data by $x_1, x_2, \\dots , x_N \\in \\mathbb{R}^d$, and outpu data by $t_1, t_2, \\dots, t_N \\in \\mathbb{R}^{d_{out}}$. \n",
    "\n",
    "If we consider multiclass classification problem with $C$ classes, $t_1, t_2 \\dots, t_N \\in \\left\\{0,1\\right\\}$, where we employ 1-of-$C$ coding scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 The representation of neural networks\n",
    "\n",
    "We consider $L$-layer feedforward neural networks (feedforward neural networks with $L-1$ hidden layer) such as one shown in the following figure: \n",
    "@@@\n",
    "\n",
    "For simplicity, we assume that there is no skip-layer connection, i.e.,  each elements in a layer is connected only to elements in its adjacent layers. \n",
    "\n",
    "In this section, we write down the definition of functions representing neural networks. Specifically, we decompose the function representing the neural network, so that we can extract contributions of parameters from a specified layer, for calculation of gradient in the next section, where we want differentiation with respect to parameters from each layer. \n",
    "\n",
    "### 2.1 The number of elements and the dimension\n",
    "\n",
    "Let $n_l \\in \\mathbb{N}$ be the number of elements in the $l$th layer. \n",
    "It follows that $n_0 = d$, $n_L = d_{out}$.\n",
    "\n",
    "### 2.2 Activation functions\n",
    "\n",
    "Again for somplicity, we assume that activation functions for $l$th-layer with $l=1,2,\\dots,L-1$ are all \n",
    "$h : \\mathbb{R} \\rightarrow \\mathbb{R}$. $h$ can be, for example, logstic sigmoid function, tanh, ReLU and so on.\n",
    "Also, we define $h^{(l)}$ by\n",
    "\n",
    "\\begin{equation}\n",
    "    h^{(l)} : \\mathbb{R}^{n_l} \\rightarrow \\mathbb{R}^{n_l}, \\ \\ \n",
    "    h^{(l)}(a) = (h(a_1), h(a_2), \\dots, h(a_{n_l}))^T \\ \\in \\mathbb{R}^{n_l}.\n",
    "\\end{equation}\n",
    "\n",
    "Let us denote the activation function for the output layer by\n",
    "$g : \\mathbb{R}^{n_L} \\rightarrow \\mathbb{R}^{n_L}$. \n",
    "\n",
    "Concretely, $g$ is chosen as below:\n",
    "* For binary classification, $g$ can be logistic sigmoid function. \n",
    "* For multiclass classification, $g$ can be softmax function.\n",
    "* For regression, $g$ can be an identity map.\n",
    "\n",
    "\n",
    "### 2.3 The input and output of each layer\n",
    "\n",
    "Let us define\n",
    "* $z^{(l)} \\in \\mathbb{R}^{n_l}$ : the output from the $l$-th layer\n",
    "* $a^{(l)} \\in \\mathbb{R}^{n_{l+1}}$ : the input to the $l+1$\n",
    "\n",
    "Then, \n",
    "\\begin{equation}\n",
    "    z^{(l)} = h^{(l)}(a^{(l-1)})\n",
    "\\end{equation}\n",
    "follows. \n",
    "\n",
    "Let us also define parameters as follows\n",
    "* $w^{(l)}_{i,j}$ : the weight of $j$-th element of $l$-th layer on the $i$-th element of$(l+1)$-th layer. \n",
    "* $b^{(l)}_{i}$ : the bias of $i$-th element of $(l+1)$-th layer.\n",
    "\n",
    "If we define \n",
    "$w^{(l)} = (w^{(l)}_{i,j})_{i,j}$ ($n_{l+1} \\times n_{l}$ matrix) and  \n",
    "$b^{(l)} = (b^{(l)}_{1}, b^{(l)}_{2}, \\dots, b^{(l)}_{n_{l+1}})^T$, \n",
    "then we have\n",
    "\n",
    "\\begin{equation}\n",
    "    a^{(l)} = w^{(l)} z^{(l)} + b^{(l)}\n",
    "\\end{equation}\n",
    "\n",
    "For later convenience, we denote this mapping by \n",
    "\\begin{equation}\n",
    "    A^{(l)}_{\\theta^{(l)}} : \\mathbb{R}^{n_{l}} \\rightarrow \\mathbb{R}^{n_{l+1}}  , \\ \\ A^{(l)}_{\\theta^{(l)} }(z) := w^{(l)}z + b^{(l)}\n",
    "\\end{equation}, \n",
    "where $\\theta^{(l)} := (w^{(l)}, b^{(l)})$.\n",
    "\n",
    "### 2.4 The whole network\n",
    "\n",
    "Let a function $f_{\\theta} : \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{d_{out}}$ be the function that represent the whole neural network we defined above. \n",
    "\n",
    "From the definitions above, it can be written as \n",
    "\n",
    "\\begin{equation}\n",
    "    f_{\\theta} = g \\circ A^{(L-1)}_{\\theta^{(L-1)}} \\circ h^{(L-1)} \\circ A^{(L-2)}_{\\theta^{(L-2)}} \\circ \\cdots \n",
    "        A^{(l+1)}_{\\theta^{(l+1)}} \\circ h^{(l+1)} \\circ A^{(l)}_{\\theta^{(l)}} \\circ h^{(l)} \\circ A^{(l-1)}_{\\theta^{(l-1)}} \n",
    "        \\circ \\cdots \\circ h^{(1)} \\circ A^{(0)}_{\\theta^{(0)}} . \n",
    "\\end{equation}\n",
    "\n",
    "Further, we can decompose the function as \n",
    "\\begin{equation}\n",
    "    f_{\\theta} = F^{(l)} \\circ A^{(l)}_{\\theta^{(l)}}  \\circ Z^{(l)} \n",
    "    \\ \\ (l = 0, \\dots, L-1), \n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    F^{(l)} &:=& g \\circ A^{(L-1)}_{\\theta^{(L-1)}} \\circ h^{(L-1)} \\circ A^{(L-2)}_{\\theta^{(L-2)}} \\circ \\cdots \n",
    "        A^{(l+1)}_{\\theta^{(l+1)}} \\circ h^{(l+1)}  \\ \\ (l = 0,1, \\dots, L-2) \\\\\n",
    "    F^{(L-1)} &:=& g \\\\\n",
    "    Z^{(l)} &:=& h^{(l)} \\circ A^{(l-1)}_{\\theta^{(l-1)}} \\circ \\cdots \\circ h^{(1)} \\circ A^{(0)}_{\\theta^{(0)}}  \\ \\ (l = 1, \\dots, L-1)\\\\\n",
    "    Z^{(0)} &:=& I\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that in the representation $f_{\\theta} = F^{(l)} \\circ A^{(l)}_{\\theta^{(l)}}  \\circ Z^{(l)} $, \n",
    "$A^{(l)}_{\\theta^{(l)}}$ is the only part that parameters for $(l+1)$-th layer appear, \n",
    "which will simplify our calculation of gradients in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Cost functions and the gradient (theory)\n",
    "\n",
    "### 3.1 Cost functions\n",
    "\n",
    "We assume that the cost function we want to minimize can be written as \n",
    "\n",
    "\\begin{equation}\n",
    "    E_{tot}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} E \\left( f_{\\theta}(x_n), t_n \\right) + \\frac{\\lambda}{N} E_{reg}(\\theta) , \n",
    "\\end{equation}\n",
    "where $\\lambda E_{reg}(\\theta)$ represents the regularization term, \n",
    "and $E$ is chosen appropriately depending on the problem: \n",
    "* For regression problem, $E$ can be $E(y,t) = \\frac{1}{2} \\| y - t \\|^2$ and so on.\n",
    "* For multiclass classification problem with $g$ being softmax functions, E can be $E(y,t) = -\\sum_{k=1}^{d_{out}} t_k \\log y_k$ (negative log likelihood).\n",
    "\n",
    "\n",
    "### 3.2 Gradient \n",
    "\n",
    "Here, we calculate the gradient of the cost function with respect to parameters $w, b$. \n",
    "We consider the regularization term separately, and concentrate on the first term. \n",
    "Because this term has the form of summation over data, we can consider each data point separately. \n",
    "\n",
    "Although we skip the derivation (if I have enough time, I will typeset the derivation later...), which is straight forward, we can obtain the following equations for $l = 0,1,\\dots, L-1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\delta^{(l)}_{i} &:=& \\sum_{k=1}^{d_{out}} \\left. \\frac{\\partial E}{\\partial y_k} \\right|_{y=f_{\\theta}(x)} \\cdot \n",
    "        \\left. \\frac{\\partial F^{(l)}_{k}}{\\partial a^{(l)}_{i}} \\right|_{a^{(l)}= (A^{(l)} \\circ Z^{(l)})(x) } \\\\\n",
    "    \\frac{\\partial}{\\partial w^{(l)}_{i,j}} E \\left( f_{\\theta}(x), t \\right) &=& \\delta^{(l)}_{i} Z^{(l)}_{j}(x) \\\\\n",
    "    \\frac{\\partial}{\\partial b^{(l)}_{i}} E \\left( f_{\\theta}(x), t \\right) &=& \\delta^{(l)}_{i} \n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, $\\delta^{(l)}_{i}$ represents how much the cost function changes when $a^{(l)}_{i}$, the input to the $i$-th element of $(l+1)$-th layer, changes slightly. \n",
    "\n",
    "Thus, to obtain the gradient, it is sufficient to calculate the quantities $\\delta^{(l)}_{i}$. \n",
    "Back propagation is an algorithm to calculate these quantities recursively. \n",
    "\n",
    "### 3.3 Back propagation\n",
    "\n",
    "In back propagation procedure, we calculate $\\delta^{(l)}_{i}$, begining from $l=L-1$, and all the way down to $l=0$. \n",
    "\n",
    "First, for $l=L-1$, we have $F^{(L-1)} = g$, and hence\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta^{(L-1)}_{i} = \\sum_{k=1}^{d_{out}} \\left. \\frac{\\partial E}{\\partial y_k} \\right|_{y=f_{\\theta}(x)} \\cdot \n",
    "        \\left. \\frac{\\partial g_{k}}{\\partial a^{(L-1)}_{i}} \\right|_{a^{(L-1)}= (A^{(L-1)} \\circ Z^{(L-1)})(x) }\n",
    "\\end{equation}\n",
    "\n",
    "This can be easily calculated for various problems : \n",
    "* For least square regression, we have $E(y,t) = \\frac{1}{2} \\| y - t \\|^2$, $g(a) = a$, and hence $\\delta^{(L-1)}_{i} = (f_{\\theta}(x) - t)_{i}$. \n",
    "* For multiclass classification with $E(y,t) = -\\sum_{k=1}^{d_{out}} t_k \\log y_k$, $g_k(a) = \\frac{e^{a_k}}{\\sum_{m} e^{a_m}}$, we obtain $\\delta^{(L-1)}_{i} = (f_{\\theta}(x) - t)_{i}$. \n",
    "\n",
    "The recursion relation can be derived straightforwardly (if I have enough time, I will typeset the derivation later, too...), and result is :\n",
    "\\begin{equation}\n",
    "    \\delta^{(l)}_{i} = h'(a^{(l)}_{i}) \\sum_{j=1}^{n_{l+2}} \\delta^{(l+1)}_{j} w^{(l+1)}_{j,i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Cost functions and the gradient (implementation)\n",
    "\n",
    "Note : To make the notation consistent with python/numpy convention, here all the indices start from 0.\n",
    "\n",
    "### 4.1 Change of notations\n",
    "\n",
    "From now on, to make the implementation simple, we treat weight $w$ and bias $b$ eqully by defining \n",
    "\n",
    "\\begin{equation}\n",
    "    W^{(l)}_{i,j} = \n",
    "    \\begin{cases}\n",
    "        b^{(l)}_{i} & (j=0) \\\\\n",
    "        w^{(l)}_{i,j-1} & (j = 1,2, \\dots, n_{l})\n",
    "    \\end{cases}\n",
    "    \\ \\ (i = 0,1, \\dots,n_{l+1} -1 ) , \n",
    "\\end{equation}\n",
    "where $W^{(l)}$ is $n_{l+1} \\times (n_{l}+1)$ matrix, and \n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{z}^{(l)}_{i} = \n",
    "    \\begin{cases}\n",
    "        1 & (i=0) \\\\\n",
    "        h(a^{(l-1)}_{i-1}) & (i = 1, \\dots, n_l)\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "With these notations, we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    a^{(l)} &=& W^{(l)} \\tilde{z}^{(l)} \\\\\n",
    "    \\tilde{z}^{(l)} &=& \n",
    "        \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            h^{(l)}(a^{(l-1)}) \n",
    "        \\end{pmatrix} \\\\\n",
    "    \\frac{\\partial}{\\partial W^{(l)}_{i,j}} E(f_{\\theta}(x),t) &=& \\delta^{(l)}_{i} \\tilde{z}^{(l)}_{j}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient for single data point\n",
    "\n",
    "For a data point $(x, t)$ we perform the following calculation : \n",
    "\n",
    "* Calculate $a, z$ for each layer (forward propagation) by \n",
    "\n",
    "\\begin{eqnarray}\n",
    "    a^{(l)} &=& W^{(l)} \\tilde{z}^{(l)}  \\ \\ (\\tilde{z}^{(0)} := (1,x^T)^T) \\ \\ (l=0, 1, \\dots, L-1) ,\\\\\n",
    "     \\tilde{z}^{(l)} &=& \n",
    "        \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            h^{(l)}(a^{(l-1)}) \n",
    "        \\end{pmatrix}\\ \\ (l=1,\\dots, L-1) \\\\\n",
    "    y &=& g(a^{(L-1)})\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Calculate $\\delta^{(L-1)}$. For examples describe in section 3.3, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta^{(L-1)}_i = (y - t)_i . \n",
    "\\end{equation}\n",
    "\n",
    "* Recursively calculate $\\delta^{(l)}_{i}$ ($l= L-2, L-1, \\dots, 1, 0$) by \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta^{(l)}_{i} = h'(a^{(l)}_{i}) \\sum_{j=1}^{n_{l+2}} \\delta^{(l+1)}_{j} w^{(l+1)}_{j,i}\n",
    "\\end{equation}\n",
    "where $i = 0,1, \\dots, n_{l+1} - 1$\n",
    "\n",
    "* Calculate the gradient using $\\delta$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\frac{\\partial}{\\partial W^{(l)}_{i,j}} E(f_{\\theta}(x),t) &=& \\delta^{(l)}_{i} \\tilde{z}^{(l)}_{j}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Vectorized implementation \n",
    "\n",
    "In the previous section, we illustrated how to calculate the gradient for a single data point. \n",
    "Although the gradient for the whole cost function (except for the regularization term) can be obtained by iterating over data points in this way and summing them up, it is desirable to vectorize the implementation, because for loop and while loop in python is slow. \n",
    "\n",
    "In this section, we will see how the vectorized implementation can be realized. The main point is to use matrix to represent data points. \n",
    "\n",
    "\n",
    "Let us define matrices as follows for $l = 0, 1, \\dots, L-1$\n",
    "* $T$ : $(N, d_{out})$ matrix, whose $(n,i)$ element corresponds to $(t_n)_{i}$. \n",
    "* $Z^{(l)}$ : $(N, n_l+1)$ matrix, whose $(n,i)$ element corresponds to $\\tilde{z}^{(l)}_{i}$ in the previous section, for the $n$-th data point $(x_n, t_n)$. Note that $Z^{(0)} = X$, where $X_{n,i} = (x_n)_{i}$. \n",
    "* $A^{(l)}$ : $(N, n_{l+1})$ matrix, whose $(n,i)$ element corresponds to $a^{(l)}_{i}$ in the previous section, for the $n$-th data point $(x_n, t_n)$. \n",
    "* $Y$ : $(N, d_{out})$ matrix, whose $(n,i)$ element corresponds to $y_i$ in the previous section for the $n$-th  data point $(x_n, t_n)$. \n",
    "* $\\Delta^{(l)}$ : $(N, n_{l+1})$ matrix, whose $(n,i)$ element corresponds to $\\delta^{(l)}_{i}$ in the previous section for the $n$-th data point $(x_n, t_n)$.\n",
    "\n",
    "Then, the procedures shown in the previous section can be rewritten as \n",
    "\n",
    "* Forward propagation, where we calculate $A^{(l)}$ and $Z^{(l)}$ ($l=0,1,\\dots, L-1$)by \n",
    "\n",
    "\\begin{eqnarray}\n",
    "    Z^{(l)} &=& \\begin{cases}\n",
    "        \\left(\\boldsymbol{1}_{N}, X \\right) & (l=0) \\\\ \n",
    "        \\left(\\boldsymbol{1}_{N} , h\\left(A^{(l-1)} \\right)\\right) & (l = 1, \\dots, L-1) \n",
    "    \\end{cases}\\\\\n",
    "    A^{(l)} &=& Z^{(l)} {W^{(l)}}^{T} \\\\\n",
    "    Y &=& g(A^{(L-1)})\n",
    "\\end{eqnarray}\n",
    "where $\\boldsymbol{1}_N := (1, 1, \\dots, 1)^T \\in \\mathbb{R}^{N}$, \n",
    "and we slightly abuse notations so that we view $h$ as acting elementwisely on $A^{(l)}$, \n",
    "and $Y = g(A^{(L-1)})$ stands for $Y_n = g(A^{L-1}_{n})$. \n",
    "\n",
    "* Calculate $\\Delta^{(L-1)}$. For examples describe in section 3.3 we get \n",
    "\n",
    "\\begin{equation}\n",
    "    \\Delta^{(L-1)} = Y - T\n",
    "\\end{equation}\n",
    "\n",
    "* Recursively calculate $\\Delta^{(l)}$ ($l= L-2, L-1, \\dots, 1, 0$) by \n",
    "\n",
    "\\begin{equation}\n",
    "    \\Delta^{(l)} = h'(A^{(l)}) \\ast \\left( \\Delta^{(l+1)} w^{(l+1)}\\right), \n",
    "\\end{equation}\n",
    "\n",
    "where we again slightly abused the definition of $h$, and $\\ast$ stands for elementwise multiplication. \n",
    "\n",
    "* Calculate the gradient using $\\Delta$ as \n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\frac{\\partial}{\\partial W^{(l)}_{i,j}} \\sum_{n=0}^{N-1} E(f_{\\theta}(x_n),t_n) \n",
    "    &=& \\sum_{n=0}^{N-1} \\Delta^{(l)}_{n,i} Z^{(l)}_{n,j} \\\\\n",
    "    &=& \\left( \\Delta^{(l)T} Z^{(l)} \\right)_{i,j}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Code\n",
    "\n",
    "Here, we implement a class representing the neural networks described above.\n",
    "Regularization term is assumed to be $E_{reg}(\\theta) = \\frac{1}{2} \\| \\theta \\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_elements = [n_0, n_1, \\dots, n_L]\n",
    "# actfunc : activation function, acting on variables elementwisely\n",
    "# actfunc : the derivative of the activation function, acting on variables elementwisely\n",
    "# actfunc_out : the activation function for the output layer\n",
    "# E : cost function\n",
    "# lam : the coefficient of the regularization term\n",
    "\n",
    "# X : input data matrix\n",
    "# T : output data matrix\n",
    "# Y : output of the neural network calculated from X\n",
    "# Wmats : the rank-1 arryay with length L, dtype object, Wmats[l] = W^{(l)}\n",
    "# Amats : the rank-1 arryay with length L, dtype object, Amats[l] = A^{(l)} \n",
    "# Zmats : the rank-1 arryay with length L, dtype object, Zmats[l] = Z^{(l)}\n",
    "class NN_proto:\n",
    "    def __init__(self,num_elements,actfunc,actfunc_diff,actfunc_out,E,lam):\n",
    "        self.num_elements = num_elements\n",
    "        self.L = L = len(self.num_elements) -1  \n",
    "        self.actfunc = actfunc\n",
    "        self.actfunc_diff = actfunc_diff\n",
    "        self.actfunc_out = actfunc_out\n",
    "        self.E = E\n",
    "        self.lam = lam\n",
    "\n",
    "        self.Wmats = np.zeros(self.L,dtype='object') \n",
    "        self.Zmats = np.zeros(self.L,dtype='object') \n",
    "        self.Amats = np.zeros(self.L,dtype='object') \n",
    "\n",
    "    # perform forward propagation\n",
    "    def fprop(self, Wmats, X):\n",
    "        self.X = X\n",
    "        self.N = len(X)\n",
    "        self.Wmats = Wmats\n",
    "        \n",
    "        self.Zmats[0] = np.concatenate(( np.ones((self.N,1)),  self.X), axis=1)\n",
    "        self.Amats[0] = self.Zmats[0] @ (self.Wmats[0].T)\n",
    "        for l in range(1,self.L):\n",
    "            self.Zmats[l] = np.concatenate( (np.ones((self.N,1)), self.actfunc(self.Amats[l-1]) ) , axis=1)\n",
    "            self.Amats[l] = self.Zmats[l] @ (self.Wmats[l].T)\n",
    "        self.Y = self.actfunc_out(self.Amats[self.L-1])\n",
    "        return self.Y\n",
    "      \n",
    "    # perform forward propagation, and output the value of the cost function\n",
    "    def cost(self, Wmats, X, T):\n",
    "        self.T = T\n",
    "        self.fprop(Wmats, X)\n",
    "        return self.E(self.Y, self.T) + \\\n",
    "            self.lam/(2*self.N)*np.sum( np.linalg.norm(Wmats[l])**2 for l in range(self.L) )\n",
    "    \n",
    "    # perform backward propagation, and output \n",
    "    def CostAndGrad(self, Wmats, X, T):\n",
    "        costval = self.cost(Wmats, X, T)\n",
    "     \n",
    "        # Dmats : the rank-1 arryay with length L, dtype object, Dmats[l] = \\Delta^{(l)} \n",
    "        self.Dmats = np.zeros(self.L, dtype='object') \n",
    "        gradmats = np.zeros(self.L, dtype='object') \n",
    "     \n",
    "        self.Dmats[self.L-1] = self.Y - self.T\n",
    "        l = self.L-2\n",
    "        while l >= 0 :\n",
    "            self.Dmats[l] = self.actfunc_diff(self.Amats[l]) * (self.Dmats[l+1] @ self.Wmats[l+1][:,1:])\n",
    "            l -= 1 \n",
    "            \n",
    "        # calculating gradient from delta\n",
    "        for l in range(0,self.L):\n",
    "            gradmats[l] = self.Dmats[l].T @ self.Zmats[l] / self.N + self.lam/self.N*self.Wmats[l]\n",
    "        return costval, gradmats    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Gradient check\n",
    "\n",
    "Because the implementation of the backward propagation is complicated, it is desirable to check whether the implementation is correct before we use the code. \n",
    "\n",
    "In this section, we compare the gradient obtained by the backward propagation and the gradient calculated from the direct numerical differentiation. \n",
    "\n",
    "The numerical differentiation is obtained as follows\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E_{tot} }{\\partial W^{(l)}_{i,j}} \\simeq \n",
    "        \\frac{ E_{tot}(W^{(l)}_{i,j}+\\varepsilon) - E_{tot}(W^{(l)}_{i,j}-\\varepsilon) }{2\\varepsilon}, \n",
    "\\end{equation}\n",
    "\n",
    "where we slightly abused the notation so that $E_{tot}(W^{(l)}_{i,j}\\pm\\varepsilon)$ means the value of the function, where all the elements, except for the $(l,i,j)$ element, of $W$ is fixed to $W^{(l')}_{i',j'}$, and the $(l,i,j)$ element is perturbed by $\\varepsilon$.\n",
    "The quantity $\\varepsilon$ is assumed to be small, and the error is $\\mathcal{O}(\\varepsilon)$, \n",
    "\n",
    "To perform the calculation, we first define a cost function and activation functions for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E(Y,T):\n",
    "    N = len(Y)\n",
    "    return -np.sum( T*np.log(Y) )/N\n",
    "\n",
    "def softmax(x):\n",
    "    x0 = np.max(x,axis=1)\n",
    "    x0 = np.reshape(x0, (len(x0),1))\n",
    "    tmp = np.sum(np.exp(x-x0),axis=1)\n",
    "    return np.exp(x-x0)/np.reshape(tmp,(len(tmp),1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_diff(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions perform numerical differentiation and the gradient check, where the function pm takes care of generating $W^{(l)}_{i,j} \\pm \\varepsilon$. \n",
    "If the implementation is correct, the output of the function gradCheck should be very small.\n",
    "\n",
    "Note : Simply deep copying \"Wmats\" by np.copy does not work, because \"Wmats\" is a numpy array with dtype being \"object\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pm(Wmats,l,i,j,ep):\n",
    "    L = len(Wmats)\n",
    "    Wmats_tmp_p = np.zeros(L,dtype='object')\n",
    "    Wmats_tmp_m = np.zeros(L,dtype='object')\n",
    "    for ll in range(0,L):\n",
    "        Wmats_tmp_p[ll] = np.copy(Wmats[ll])\n",
    "        Wmats_tmp_m[ll] = np.copy(Wmats[ll])\n",
    "    Wmats_tmp_p[l][i,j] += ep\n",
    "    Wmats_tmp_m[l][i,j] -= ep\n",
    "    return Wmats_tmp_p, Wmats_tmp_m\n",
    "    \n",
    "def gradCheck(nn, Wmats, X, T, ep):\n",
    "    gradmats_num =  np.zeros(nn.L, dtype='object')\n",
    "    for l in range(nn.L):\n",
    "        gradmats_num[l] = np.zeros((nn.num_elements[l+1],nn.num_elements[l]+1))\n",
    "        for i in range(nn.num_elements[l+1]):\n",
    "            for j in range(nn.num_elements[l]+1):\n",
    "                Wmats_tmp_p, Wmats_tmp_m = pm(Wmats,l,i,j,ep)\n",
    "                gradmats_num[l][i,j] = (nn.cost(Wmats_tmp_p,X,T) - nn.cost(Wmats_tmp_m,X,T))/(2*ep)\n",
    "    gradmats_bp = nn.CostAndGrad(Wmats, X, T)[1]\n",
    "    return np.sum(np.array([np.linalg.norm(gradmats_num[l]-gradmats_bp[l])**2  for l in range(nn.L)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check our implementation with a small neural network with randomly generated parameters and data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2076834984395087e-16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_elements = np.array([4,3,2])\n",
    "Wmats = np.array([np.random.random((3,5)), np.random.random((2,4)) ])\n",
    "X = np.random.random((2,4))\n",
    "T = np.array([[1,0],[0,1]])\n",
    "\n",
    "nn_test = NN_proto(num_elements,sigmoid,sigmoid_diff,softmax,E,lam=0.5)\n",
    "gradCheck(nn_test, Wmats, X, T, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see that the gradient calculated from the backward propagation is consistent with that calculated from numerical differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.5 Training neural networks\n",
    "\n",
    "In the previous section, we have implemented neural networks with forward and backward propagation, by which we can obtain the cost function and its gradient. \n",
    "In this section, we implement functions which train the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(NN_proto):\n",
    "    \n",
    "    ## reshaping functions. \n",
    "    ## These functions are required, because scipy minimize can take a function of a vector only.\n",
    "    # reshaping Wmats and gradmats (from a vector to matrices)\n",
    "    def reshapeW_vec2mats(self,vec):\n",
    "        mats = np.zeros(self.L,dtype='object')\n",
    "        tmp = 0\n",
    "        for l in range(self.L):\n",
    "            matlsize = self.num_elements[l+1]*(self.num_elements[l]+1)\n",
    "            mats[l] = np.reshape(vec[tmp:tmp+matlsize], ( self.num_elements[l+1], self.num_elements[l]+1 ))\n",
    "            tmp += matlsize        \n",
    "        return mats\n",
    "    # reshaping Wmats and gradmats (from matrices to a vector)\n",
    "    def reshapeW_mats2vec(self,mats):\n",
    "        vec = np.zeros( np.sum(self.num_elements[1:]*(self.num_elements[:self.L]+1)) )\n",
    "        tmp = 0\n",
    "        for l in range(self.L):\n",
    "            matlsize = self.num_elements[l+1]*(self.num_elements[l]+1)\n",
    "            vec[tmp:tmp+matlsize] = np.reshape(mats[l], matlsize)\n",
    "            tmp += matlsize\n",
    "        return vec\n",
    "    # Cost and gradient functions as a function of a vector, and returns a real number and a vector\n",
    "    def CostAndGrad_vec(self,Wvec, X, T):\n",
    "        Wmats = self.reshapeW_vec2mats(Wvec)\n",
    "        costval, gradmats = self.CostAndGrad(Wmats, X, T)\n",
    "        return costval, self.reshapeW_mats2vec(gradmats)\n",
    "    \n",
    "    def fit(self, X, T, ep = 0.01):\n",
    "        w0 = 2*ep*(np.random.random(np.sum(self.num_elements[1:]*(self.num_elements[:self.L]+1))) - 0.5)\n",
    "        ans = minimize(self.CostAndGrad_vec,x0 = w0, args =(X,T), method='CG', jac=True)\n",
    "        print(ans['fun'])\n",
    "        print(ans['success'])\n",
    "        print(ans['message'])\n",
    "        self.Wmats = self.reshapeW_vec2mats(ans['x'])\n",
    "        \n",
    "    def pred_prob(self, X):\n",
    "        return self.fprop(self.Wmats, X)\n",
    "    def pred_class(self, X):\n",
    "        self.fprop(self.Wmats,X)\n",
    "        return np.argmax(self.Y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Classifying toy data\n",
    "\n",
    "As a toy example, we use the following \"spiral\" data in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8f99390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = 3\n",
    "\n",
    "def r(t):\n",
    "    return t**0.5\n",
    "\n",
    "N = 120\n",
    "xdat = np.zeros((N,2))\n",
    "ydat = np.zeros(N,dtype='int')\n",
    "cnt = 0\n",
    "while cnt < N :\n",
    "    c = np.random.randint(0,C)\n",
    "    ydat[cnt] = c\n",
    "    tmp = 2.9*np.random.random() + 0.1\n",
    "    xdat[cnt] = np.array([r(tmp)*np.cos(tmp+c*2*np.pi/3),r(tmp)*np.sin(tmp+c*2*np.pi/3)]) + 0.1*(np.random.random(2) -0.5)\n",
    "    cnt += 1\n",
    "\n",
    "tdat = np.zeros((N,C))\n",
    "for c in range(C):\n",
    "    tdat[:,c] = (ydat == c)\n",
    "    plt.plot(xdat[np.where(ydat==c)][:,0],xdat[np.where(ydat==c)][:,1],'.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a 2-layer neural network with logistic sigmoid activation function and softmax output function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002163159841960683\n",
      "True\n",
      "Optimization terminated successfully.\n"
     ]
    }
   ],
   "source": [
    "num_elements = np.array([2,3,3])\n",
    "\n",
    "nn_2Dspiral = NN(num_elements,sigmoid,sigmoid_diff,softmax,E,lam=0.0)\n",
    "nn_2Dspiral.fit(xdat,tdat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of the classifier can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAoKIiAB+14p5AAAAC0lEQVQKCCIiEvr/VZ99aNdNd6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8bf0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num0 = 50\n",
    "num1 = 51\n",
    "x0 = np.linspace(-2.0,2.0,num0)\n",
    "x1 = np.linspace(-2.0,2.0,num1)\n",
    "X0,X1 = np.meshgrid(x0,x1)\n",
    "\n",
    "Xtest = np.zeros((np.size(X0),2))\n",
    "Xtest[:,0] = np.reshape(X0,np.size(X0))\n",
    "Xtest[:,1] = np.reshape(X1,np.size(X1))\n",
    "                        \n",
    "pred_label = nn_2Dspiral.pred_class(Xtest)\n",
    "pred_label = np.reshape(pred_label,(num1,num0))\n",
    "\n",
    "plt.pcolormesh(X0,X1, pred_label)\n",
    "for c in range(C):\n",
    "    tdat[:,c] = (ydat == c)\n",
    "    plt.plot(xdat[np.where(ydat==c)][:,0],xdat[np.where(ydat==c)][:,1],'.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Classifying hand written digits\n",
    "\n",
    "As another example, here we apply the neural network to hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data : 1347\n",
      "Test data : 450\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "dat_train, dat_test, label_train, label_test = \\\n",
    "cross_validation.train_test_split(digits.data,digits.target)\n",
    "t_train = np.zeros((len(label_train),10))\n",
    "for c in range(10):\n",
    "    t_train[:,c] = (label_train == c)\n",
    "\n",
    "print(f\"Training data : {len(dat_train)}\")\n",
    "print(f\"Test data : {len(dat_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018703275243978196\n",
      "True\n",
      "Optimization terminated successfully.\n"
     ]
    }
   ],
   "source": [
    "num_elements = np.array([64,35,10])\n",
    "nn_mnist = NN(num_elements, sigmoid, sigmoid_diff, softmax, E, lam=0.01)\n",
    "nn_mnist.fit(dat_train,t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984444444444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        48\n",
      "          1       1.00      0.97      0.99        36\n",
      "          2       0.98      0.98      0.98        46\n",
      "          3       0.98      0.98      0.98        52\n",
      "          4       0.98      1.00      0.99        54\n",
      "          5       0.98      0.98      0.98        45\n",
      "          6       0.97      1.00      0.99        38\n",
      "          7       1.00      1.00      1.00        41\n",
      "          8       0.97      0.95      0.96        41\n",
      "          9       0.98      0.98      0.98        49\n",
      "\n",
      "avg / total       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_test_pred = nn_mnist.pred_class(dat_test)\n",
    "ac_score = metrics.accuracy_score(label_test, label_test_pred)\n",
    "cl_report = metrics.classification_report(label_test, label_test_pred)\n",
    "print(ac_score)\n",
    "print(cl_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
