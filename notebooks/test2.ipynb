{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test \n",
    "\n",
    "lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import eigh\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rc(\"savefig\",dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Optimizing hyper parameters \n",
    "\n",
    "So far, we have assumed hyper parameters contained in the kernel function is fixed. \n",
    "In this section, we describe how they can be determined from the training data. \n",
    "Specifically, we choose the set of hyper parameters which maximizes the marginal likelihood $p(t|\\theta)$. \n",
    "\n",
    "### 4.1 Marginal likelihood\n",
    "\n",
    "As described in the previous section, we have \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    & p(\\boldsymbol{t}|\\theta) = \\mathcal{N}(\\boldsymbol{t} | 0, C) \\\\\n",
    "    & C = K + \\beta^{-1} I_N \\\\\n",
    "    & K = (k(x_i,x_j))_{i,j = 1, \\dots, N} \\\\\n",
    "    & k(a,b) = \\theta_0 \\exp\\left( -\\frac{\\theta_1}{2} \\| a- b\\|^2 \\right) + \\theta_2 + \\theta_3 a^T b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the log marginal likelihood is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\log p(\\boldsymbol{t}|\\theta) = -\\frac{1}{2} \\log ({\\rm det} C) \n",
    "        -\\frac{1}{2} \\boldsymbol{t}^T C^{-1} \\boldsymbol{t} -\\frac{N}{2} \\log (2\\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We regard it as a function of $\\theta$, and maximize this function. \n",
    "\n",
    "### 4.2 Gradient\n",
    "\n",
    "To perform the optimization, we also need the gradient, which is given by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial \\theta_i} \\log p(\\boldsymbol{t}|\\theta) \n",
    "    = -\\frac{1}{2} Tr \\left(C^{-1} \\frac{\\partial C}{\\partial \\theta_i} \\right)\n",
    "        + \\frac{1}{2} \\boldsymbol{t}^T C^{-1} \\frac{\\partial C}{\\partial \\theta_i} C^{-1} \\boldsymbol{t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Although not written explicitly in the textbook, we also perform the optimization over $\\beta$ simultaneously.\n",
    "Also note that, because we want to use \"scipy.optimize.minimize\", the sign is inverted (See the \"return\" part.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4 Optimizing hyper parameters  (rmを削除)\n",
    "\n",
    "So far, we have assumed hyper parameters contained in the kernel function is fixed. \n",
    "In this section, we describe how they can be determined from the training data. \n",
    "Specifically, we choose the set of hyper parameters which maximizes the marginal likelihood $p(t|\\theta)$. \n",
    "\n",
    "### 4.1 Marginal likelihood\n",
    "\n",
    "As described in the previous section, we have \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    & p(\\boldsymbol{t}|\\theta) = \\mathcal{N}(\\boldsymbol{t} | 0, C) \\\\\n",
    "    & C = K + \\beta^{-1} I_N \\\\\n",
    "    & K = (k(x_i,x_j))_{i,j = 1, \\dots, N} \\\\\n",
    "    & k(a,b) = \\theta_0 \\exp\\left( -\\frac{\\theta_1}{2} \\| a- b\\|^2 \\right) + \\theta_2 + \\theta_3 a^T b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the log marginal likelihood is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\log p(\\boldsymbol{t}|\\theta) = -\\frac{1}{2} \\log (det C) \n",
    "        -\\frac{1}{2} \\boldsymbol{t}^T C^{-1} \\boldsymbol{t} -\\frac{N}{2} \\log (2\\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We regard it as a function of $\\theta$, and maximize this function. \n",
    "\n",
    "### 4.2 Gradient\n",
    "\n",
    "To perform the optimization, we also need the gradient, which is given by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial \\theta_i} \\log p(\\boldsymbol{t}|\\theta) \n",
    "    = -\\frac{1}{2} Tr \\left(C^{-1} \\frac{\\partial C}{\\partial \\theta_i} \\right)\n",
    "        + \\frac{1}{2} \\boldsymbol{t}^T C^{-1} \\frac{\\partial C}{\\partial \\theta_i} C^{-1} \\boldsymbol{t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Although not written explicitly in the textbook, we also perform the optimization over $\\beta$ simultaneously.\n",
    "Also note that, because we want to use \"scipy.optimize.minimize\", the sign is inverted (See the \"return\" part.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4 Optimizing hyper parameters  (mathrmを使用)\n",
    "\n",
    "So far, we have assumed hyper parameters contained in the kernel function is fixed. \n",
    "In this section, we describe how they can be determined from the training data. \n",
    "Specifically, we choose the set of hyper parameters which maximizes the marginal likelihood $p(t|\\theta)$. \n",
    "\n",
    "### 4.1 Marginal likelihood\n",
    "\n",
    "As described in the previous section, we have \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    & p(\\boldsymbol{t}|\\theta) = \\mathcal{N}(\\boldsymbol{t} | 0, C) \\\\\n",
    "    & C = K + \\beta^{-1} I_N \\\\\n",
    "    & K = (k(x_i,x_j))_{i,j = 1, \\dots, N} \\\\\n",
    "    & k(a,b) = \\theta_0 \\exp\\left( -\\frac{\\theta_1}{2} \\| a- b\\|^2 \\right) + \\theta_2 + \\theta_3 a^T b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the log marginal likelihood is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\log p(\\boldsymbol{t}|\\theta) = -\\frac{1}{2} \\log (\\mathrm{det} C) \n",
    "        -\\frac{1}{2} \\boldsymbol{t}^T C^{-1} \\boldsymbol{t} -\\frac{N}{2} \\log (2\\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We regard it as a function of $\\theta$, and maximize this function. \n",
    "\n",
    "### 4.2 Gradient\n",
    "\n",
    "To perform the optimization, we also need the gradient, which is given by \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial \\theta_i} \\log p(\\boldsymbol{t}|\\theta) \n",
    "    = -\\frac{1}{2} Tr \\left(C^{-1} \\frac{\\partial C}{\\partial \\theta_i} \\right)\n",
    "        + \\frac{1}{2} \\boldsymbol{t}^T C^{-1} \\frac{\\partial C}{\\partial \\theta_i} C^{-1} \\boldsymbol{t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Although not written explicitly in the textbook, we also perform the optimization over $\\beta$ simultaneously.\n",
    "Also note that, because we want to use \"scipy.optimize.minimize\", the sign is inverted (See the \"return\" part.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
